---
format: 
  revealjs:
    footer: "Packages for forecasting --- [cmu-delphi.github.io/cste-forecast-workshop-2023]{.smaller}"
    logo: "gfx/delphi.jpg"
    embed-resources: false
    chalkboard: true
    width: 1280
    height: 720
    theme: [default, themer.scss]
    fig-format: svg
    html-math-method: mathjax
execute: 
  cache: true
editor: source
---

```{r}
#| fig-align: center
#| fig-format: svg
primary <- "#a8201a"
secondary <- "#f9c80e"
tertiary <- "#2a76dd"
fourth_colour <- "#311847"
library(epiprocess)
library(epipredict)
suppressMessages(library(tidyverse))
x <- archive_cases_dv_subset
x_latest <- epix_as_of(x, max_version = max(x$DT$version))
self_max = max(x$DT$version)
versions = seq(as.Date("2020-06-01"), self_max - 1, by = "1 month")
snapshots_all <- map_dfr(versions, function(v) { 
  epix_as_of(x, max_version = v) %>% mutate(version = v)}) %>%
  bind_rows(x_latest %>% mutate(version = self_max)) %>%
  mutate(latest = version == self_max)
snapshots <- snapshots_all %>% 
  filter(geo_value %in% c("ca", "fl"))
```

```{r}
#| include: false
#| label: cover-art
ggplot(snapshots_all %>% 
         arrange(geo_value, version, time_value) %>% 
         filter(!latest),
       aes(x = time_value, y = percent_cli)) +  
  geom_line(aes(color = factor(version), group = interaction(geo_value, version))) + 
  #geom_vline(aes(color = factor(version), xintercept = version), lty = 3, 
  #           size = 0.5) +
  scale_x_date(minor_breaks = "month", labels = NULL) +
  labs(x = "", y = "") + 
  theme_void() +
  coord_cartesian(xlim = as.Date(c("2020-10-01", NA)), ylim = c(-5, NA)) +
  scale_color_viridis_d(option = "B", end = .8) +
  theme(legend.position = "none", panel.background = element_blank()) +
  geom_line(
    data = snapshots %>% filter(latest),
    aes(x = time_value, y = percent_cli, group = geo_value), 
    inherit.aes = FALSE, color = "black")
```



:::: {.columns}
::: {.column width="20%"}

:::
::: {.column width="80%"}
##  `{epiprocess}` and `{epipredict}` {background-image="index_files/figure-revealjs/cover-art-1.svg" background-position="bottom"}

### `R` packages for signal processing and forecasting

<br>

#### Daniel J. McDonald and Logan C. Brooks
#### and CMU's Delphi Group

CSTE Workshop on Infectious Disease Forecasting --- 25 June 2023
:::
::::

## Background

* Covid-19 Pandemic required quickly implementing forecasting systems.

* Basic processing---[outlier detection]{.primary}, [reporting issues]{.secondary}, [geographic granularity]{.tertiary}---implemented in parallel / error prone

* Data revisions complicate evaluation

* Simple models often outperformed complicated ones

* Custom software not easily adapted / improved by other groups

* Hard for public health actors to borrow / customize community techniques



## Packages under development

```{mermaid}
%%| fig-width: 8
%%| fig-height: 6
flowchart LR
  A("{epidatr}") --> C("{epiprocess}")
  B("{epidatpy}") --> C
  D("{covidcast}") --> C
  C --> E("{epipredict}")
```

# `{epiprocess}`
[Tools for signal processing]{.primary}

## Basic processing operations and data structures

::: {.incremental}

* General EDA for "panel data"
* Calculate rolling statistics
* Fill / impute gaps
* Examine correlations
* Store revision history smartly
* Inspect revision patterns
* Find / correct outliers

:::

## `epi_df`: snapshot of a data set

* a tibble required columns, `geo_value` and `time_value`.
* arbitrary additional columns containing [measured values]{.primary}
* additional [keys]{.primary} to index (`age_group`, `ethnicity`, etc.)

::: {.callout-note}
## `epi_df`

Represents a [snapshot]{.primary} that
contains the most [up-to-date values]{.primary} of the signal variables, [as of]{.primary} a given time.
:::

### `epi_df`: snapshot of a data set

```{r}
edf <- case_death_rate_subset
edf
```

## Sliding examples on `epi_df`

### Correlations at different lags

```{r, echo=TRUE}
cor0 <- epi_cor(edf, case_rate, death_rate, cor_by = time_value)
cor14 <- epi_cor(edf, case_rate, death_rate, cor_by = time_value, dt1 = -14)
```

```{r}
#| fig-align: center
rbind(
  cor0 %>% mutate(lag = 0),
  cor14 %>% mutate(lag = 14)
) %>%
  mutate(lag = as.factor(lag)) %>%
  ggplot(aes(x = time_value, y = cor)) +
  geom_hline(yintercept = 0, size = 1.5) +
  geom_line(aes(color = lag), size = 1.5) +
  theme_bw(16) +
  scale_color_brewer(palette = "Set1") +
  scale_x_date(minor_breaks = "month", date_labels = "%b %Y") +
  labs(x = "Date", y = "Correlation", col = "Lag")
```


## Sliding examples on `epi_df`

### Growth rates

```{r, echo=TRUE}
edf <- filter(edf, geo_value %in% c("ut", "ca")) %>%
  group_by(geo_value) %>%
  mutate(gr_cases = growth_rate(time_value, case_rate, method = "trend_filter"))
```

```{r}
#| fig-align: center
ggplot(edf, aes(x = time_value, y = gr_cases)) +
  geom_hline(yintercept = 0, size = 1.5) +
  geom_line(aes(col = geo_value), size = 1.5) +
  geom_hline(yintercept = 0) +
  theme_bw(16) +
  scale_y_continuous(labels = scales::label_percent()) +
  scale_color_manual(values = c(3, 6)) +
  scale_x_date(minor_breaks = "month", date_labels = "%b %Y") +
  labs(x = "Date", y = "Growth rate", col = "State")
```

## `epi_archive`: collection of `epi_df`s

* full version history of a data set
* acts like a bunch of `epi_df`s --- but stored [compactly]{.primary}
* Allows similar funtionality as `epi_df` but using only [data that would have been available at the time]{.primary}

::: {.callout-note}
## Revisions

Epidemiology data gets revised frequently. (Happens in Economics as well.) 

* We may want to use the data [as it looked in the past]{.primary} 
* or we may want to examine [the history of revisions]{.primary}.
:::


## Revision patterns

```{r}
ggplot(snapshots %>% filter(!latest),
            aes(x = time_value, y = percent_cli)) +  
  geom_line(aes(color = factor(version))) + 
  geom_vline(aes(color = factor(version), xintercept = version), lty = 3) +
  facet_wrap(~ geo_value, scales = "free_y", nrow = 1) +
  scale_x_date(minor_breaks = "month", date_labels = "%b %Y") +
  labs(x = "", y = "% of doctor's visits with\n Covid-like illness") + 
  theme_bw(base_size = 24) +
  scale_color_viridis_d(option = "B", end = .8) +
  theme(legend.position = "none") +
  geom_line(data = snapshots %>% filter(latest),
               aes(x = time_value, y = percent_cli), 
               inherit.aes = FALSE, color = "black")
```

## Latency investigation

How stale is my data over space and time?

There are (lots) of possible definitions.

[Nominal latency]{.primary}
: the difference between the max `time_value` available for some location, as of some version

[Nonmissing latency]{.primary}
: nominal + recorded `NA`s and skipped `time_value`s are latent

[Nonmissing nonzero latency]{.secondary}
: nonmissing + zeros are latent

[Nonmissing nonduplicate latency]{.primary}
: nonmissing + duplications of the most recent non-`NA` values are latent


## Latency investigation

```{r}
#| eval: false
#| echo: true
#| code-line-numbers: "|5|6|8-9"
pos_first_true <- function(x) match(TRUE, rev(x), length(x) + 1) - 1

na0_latency <- function(dat, group_key, ref_time_value) {
  group_by(dat, geo_value) %>%
    complete(time_value = full_seq(time_value, period = 1L)) %>%
    mutate(not_na0 = !(is.na(admissions) | admissions == 0L)) %>%
    summarize(
      nominal_latency = as.integer(ref_time_value - max(time_value)),
      na0_latency = pos_first_true(not_na0) + nominal_latency
    )
}
```

## Latency investigation, > 2 days

```{r, dev = "png"}
hhs_na0_latency <- readRDS("hhs_na0_latency.rds")
hhs_na0_latency %>%
  mutate(na0_latency = case_when(
    na0_latency <= 2 ~ NA,
    TRUE ~ na0_latency
  )) %>%
  ggplot(aes(version, geo_value, fill = na0_latency)) +
  geom_raster() +
  scale_fill_viridis_c(
    na.value = "transparent", name = "NMNZ latency",
    guide = guide_colorbar(barheight = 12)) +
  theme_bw(16) +
  scale_x_date(date_labels = "%b %Y", expand = expansion()) +
  labs(xlab = "Version", ylab = "") 
```

# `{epipredict}`
[A forecasting framework]{.primary}

## `{epipredict}`
### Building up modular components.

1. [Preprocessor:]{.primary} do things to the data before model training
2. [Trainer:]{.primary} train a model on data, resulting in an object
3. [Predictor:]{.primary} make predictions, using a fitted model object
4. [Postprocessor:]{.primary} do things to the predictions before returning


::: {.callout-important icon=false}
##

A very specialized plug-in to [`{tidymodels}`](https://tidymodels.org)
:::


## `{epipredict}` 

::: {.incremental}

* Flatline forecaster
* AR-type models
* Backtest using the versioned data
* Easily create features
* Modify / transform / calibrate forecasts 
* Quickly pivot to new tasks
* Highly customizable for advanced users 

:::

## Canned forecasters that work out of the box.

But, you can adjust [a lot]{.secondary} of options

### We currently provide:

- Baseline flat-line forecaster
- Autoregressive-type forecaster
- Autoregressive-type classifier
- "Smooth" autoregressive-type forecaster

## Canned forecasters that work out of the box.

But, you can adjust [a lot]{.secondary} of options

### Making dumb (but useful!) forecasts in epidemiology

* We want to predict 
  - new hospitalizations $y$, 
  - $h$ days ahead, 
  - at many locations $j$.

* We're going to make a new forecast each week.


## Canned forecasters that work out of the box.

But, you can adjust [a lot]{.secondary} of options

### Flatline forecaster

For each location, predict 
$$\hat{y}_{j,\ i+h} = y_{j,\ i}$$

## Canned forecasters that work out of the box.

But, you can adjust [a lot]{.secondary} of options

### AR forecaster

Use an AR model with an extra feature, e.g.:
$$\hat{y}_{j,\ i+h} = \mu + a_0 y_{j,\ i} + a_7 y_{j,\ i-7} + b_0 x_{j,\ i} + b_7 x_{j,\ i-7}$$



## Basic autoregressive forecaster

* Predict `death_rate`, 1 week ahead, with `0,7,14` day lags of `cases` and `deaths`. 
* Use `lm` for estimation. Also create intervals.

```{r}
#| echo: true
#| warning: false
jhu <- case_death_rate_subset # grab some built-in data
canned <- arx_forecaster(
  epi_data = jhu, 
  outcome = "death_rate", 
  predictors = c("case_rate", "death_rate")
)
```

The output is a model object that could be reused in the future, along with the predictions for 7 days from now.

## Adjust lots of built-in options

```{r}
#| echo: true
#| eval: false
#| code-line-numbers: "|4|5|7|8-12|13|14"
rf <- arx_forecaster(
  epi_data = jhu, 
  outcome = "death_rate", 
  predictors = c("case_rate", "death_rate", "fb-survey"),
  trainer = parsnip::rand_forest(mode = "regression"), # use {ranger}
  args_list = arx_args_list(
    ahead = 14, # 2-week horizon
    lags = list(
      case_rate = c(0:4, 7, 14), 
      death_rate = c(0, 7, 14), 
      `fb-survey` = c(0:7, 14)
    ),
    levels = c(0.01, 0.025, 1:19/20, 0.975, 0.99), # 23 ForecastHub quantiles
    quantile_by_key = "geo_value" # vary quantile forecasts by location
  )
)
```



## Do (almost) anything manually

```{r}
#| echo: true
#| eval: false
#| code-line-numbers: "1-6|8-13|15-17|19-26"
# A preprocessing "recipe" that turns raw data into features / response
r <- epi_recipe(jhu) %>%
  step_epi_lag(case_rate, lag = c(0, 1, 2, 3, 7, 14)) %>%
  step_epi_lag(death_rate, lag = c(0, 7, 14)) %>%
  step_epi_ahead(death_rate, ahead = 14) %>%
  step_epi_naomit()

# A postprocessing routine describing what to do to the predictions
f <- frosting() %>%
  layer_predict() %>%
  layer_threshold(.pred, lower = 0) %>% # predictions/intervals should be non-negative
  layer_add_target_date(target_date = max(jhu$time_value) + 14) %>%
  layer_add_forecast_date(forecast_date = max(jhu$time_value))

# Bundle up the preprocessor, training engine, and postprocessor
# We use quantile regression
ewf <- epi_workflow(r, quantile_reg(tau = c(.1, .5, .9)), f)

# Fit it to data (we could fit this to ANY data that has the same format)
trained_ewf <- ewf %>% fit(jhu)

# examines the recipe to determine what we need to make the prediction
latest <- get_test_data(r, jhu)

# we could make predictions using the same model on ANY test data
preds <- trained_ewf %>% predict(new_data = latest)
```


## Packages are under active development {.smaller}


### Thanks:

```{r qr-codes}
#| include: false
#| fig-format: png
# Code to generate QR codes to link to any external sources
qrdat <- function(text, ecl = c("L", "M", "Q", "H")) {
  x <- qrcode::qr_code(text, ecl)
  n <- nrow(x)
  s <- seq_len(n)
  tib <- tidyr::expand_grid(x = s, y = rev(s))
  tib$z <- c(x)
  tib
}
qr1 <- qrdat("https://cmu-delphi.github.io/epiprocess/")
qr2 <- qrdat("https://cmu-delphi.github.io/epipredict/")
qr3 <- qrdat("https://cmu-delphi.github.io/delphi-tooling-book/")
qr4 <- qrdat("https://cmu-delphi.github.io/cste-forecast-workshop-2023/")
plot_qr <- function(dat, title, fill = "black") {
  ggplot(dat, aes(x, y, fill = z)) +
    geom_raster() +
    ggtitle(title) +
    coord_equal(expand = FALSE) +
    scale_fill_manual(values = c("white", fill), guide = "none") +
    theme_void(base_size = 18) +
    theme(plot.title = element_text(hjust = .5, size = 30))
}
ggsave(plot = plot_qr(qr1, "{epipredict}"), "gfx/qr-epipredict.png")
ggsave(plot = plot_qr(qr2, "{epiprocess}"), "gfx/qr-epiprocess.png")
ggsave(plot = plot_qr(qr3, "Tooling book"), "gfx/qr-tooling-book.png")
ggsave(plot = plot_qr(qr4, "This talk"), "gfx/qr-this-talk.png")
```

:::: {.columns}
::: {.column width="50%"}
- The whole [CMU Delphi Team](https://delphi.cmu.edu/about/team/) (across many institutions)
- Optum/UnitedHealthcare, Change Healthcare.
- Google, Facebook, Amazon Web Services.
- Quidel, SafeGraph, Qualtrics.
- Centers for Disease Control and Prevention.
- Council of State and Territorial Epidemiologists
- National Sciences and Engineering Research Council of Canada
:::

::: {.column width="50%"}

![](gfx/qr-epiprocess.png){width="300px"}
![](gfx/qr-epipredict.png){width="300px"}
![](gfx/qr-tooling-book.png){width="300px"}
![](gfx/qr-this-talk.png){width="300px"}

:::

::::

::: {layout-row=1 fig-align="center"}
![](gfx/delphi.jpg){height="100px"}
![](gfx/berkeley.jpg){height="100px"}
![](gfx/cmu.jpg){height="100px"}
![](gfx/ubc.jpg){width="250px"}
![](gfx/usc.jpg){width="250px"}
![](gfx/stanford.jpg){width="250px"}
:::


