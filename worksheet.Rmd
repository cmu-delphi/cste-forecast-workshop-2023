---
title: "Delphi CSTE 2023 Worksheet"
author: DJM and LCB
date: 2023-06-24
output: html_document
---

Our overall goal with this worksheet is to produce **state death forecasts**.
We'll load a bunch of data, examine some necessary preprocessing routines,
explore the possible features, estimate a forecaster, and finally, compare the
results to the observed data.

# Packages and data

First, we'll load some libraries and data. The data is saved in this repo, for
offline use. But one could use `{epidatr}` to download it if desired.

<details> <summary> Data documentation, license, attribution </summary>

See API data documentation available
[here](https://cmu-delphi.github.io/delphi-epidata/), and licensing and citation
info
[here](https://cmu-delphi.github.io/delphi-epidata/api/covidcast_licensing.html).
The data used here is all available under [CC BY
4.0](https://creativecommons.org/licenses/by/4.0/).

- CTIS ("fb-survey") and doctor visits data: Data from Delphi COVIDcast.
  Obtained via the Delphi Epidata API.
  https://cmu-delphi.github.io/delphi-epidata/api/covidcast.html
- JHU-CSSE data: COVID-19 Data Repository by the Center for Systems Science and
  Engineering (CSSE) at Johns Hopkins University.

Data selections and modifications are described in the code chunks in this
document and the linked API documentation.

</details>

```{r load-libraries, message=FALSE, warning=FALSE}
library(epidatasets)
library(epiprocess)
library(epipredict)
library(dplyr)
library(tibble)
library(ggplot2)
library(purrr)

theme_set(theme_bw())
```

Here's the code we ran to download the data. This likely requires an API key.
If you try it, it should spit out instructions.

```{r download-data, eval=FALSE, echo=FALSE}
library(epidatr)
masking <- covidcast(
  data_source = "fb-survey",
  signals = "smoothed_wwearing_mask_7d",
  time_type = "day",
  geo_type = "state",
  time_values = epirange(20210604, 20211231),
  geo_values = "*")  %>%
  fetch() %>%
  select(geo_value, time_value, masking = value)


distancing <- covidcast(
  data_source = "fb-survey",
  signals = "smoothed_wothers_distanced_public",
  time_type = "day",
  geo_type = "state",
  time_values = epirange(20210604, 20211231),
  geo_values = "*")  %>%
  fetch() %>%
  select(geo_value, time_value, distancing = value)

cases <- covidcast(
  data_source = "jhu-csse",
  signals = "confirmed_incidence_num",
  time_type = "day",
  geo_type = "state",
  time_values = epirange(20210604, 20211231),
  geo_values = "*") %>%
  fetch() %>%
  select(geo_value, time_value, cases = value)

deaths <- covidcast(
  data_source = "jhu-csse",
  signals = "deaths_incidence_num",
  time_type = "day",
  geo_type = "state",
  time_values = epirange(20210604, 20211231),
  geo_values = "*") %>%
  fetch() %>%
  select(geo_value, time_value, deaths = value)

dv <- covidcast(
  data_source = "doctor-visits",
  signals = "smoothed_adj_cli",
  time_type = "day",
  geo_type = "state",
  time_values = epirange(20210604, 20211231),
  geo_values = "*") %>%
  fetch() %>%
  select(geo_value, time_value, dv_cli = value)

byvec <- c("geo_value", "time_value")

dat <- left_join(cases, deaths, by = byvec) %>%
  left_join(masking, by = byvec) %>%
  left_join(distancing, by = byvec) %>%
  left_join(dv, by = byvec)

dat <- as_epi_df(dat) %>%
  # remove territories and dc because CTIS signals aren't always available
  # for these
  filter(!(geo_value %in% c("as", "dc", "gu", "mp", "pr", "vi"))) 
saveRDS(dat, here::here("data", "combined_signals.rds"))
```

OK. Here we'll just load the saved data.

```{r}
dat <- readRDS(here::here("data", "combined_signals.rds"))
```

# Exploring signals and processing

Take a look at the dataset

```{r}
dat
```

It's already in `epi_df` format, so some things are easy to read.
For instance, the data version is in June of 2023, while the data end on 
`r max(dat$time_value)`, so reporting errors and revisions are removed from
this task. 

This simplification makes certain things easier, but is not 
advised for actually evaluating real-time forecasting models.

The signals that are available are:

* `cases`: incident cases as reported by JHU
* `deaths`: incident deaths as reported by JHU
* `masking`: a measure of adherence to masking recommendations as collected with the CTIS Survey
* `distancing`: a measure of adherence to social distancing recommendations as collected with the CTIS Survey
* `dv_cli`: estimated percentage of outpatient visits due to Covid-like-illness

The first two (cases and deaths) are in counts. The others have been adjusted
to be commensurate across locations, and also smoothed.

## Preparing incommensurate data

In this section, we'll perform 2 data transformations for both cases and deaths.
This has 2 purposes:

1. We've found that it's more accurate to train forecasters on rates instead of
counts. This puts different locations on the same scale, allowing statistical 
models to borrow strength.
2. When predicting totals over a week (like case and death forecasts submitted to the COVID-19 Forecast Hub), we've found it desirable to train and predict using weekly averages, then scale up, rather than training directly on the total. This smooths out any spikes and also deals directly with day of the week effects. Importantly, it also allows us to use **every day** as a row in our training data rather than **only totals for each Monday**. The result is that we have 7x as much data.

**Note:**
While we're going to "preprocess" our data before using it for forecasting,
these steps could be done directly within the `{epipredict}` framework. 
This exercise will use a "canned" forecaster, rather than the general implementation, so this functionality isn't allowed. And additionally, it's a 
good exercise to use `{epiprocess}`.

First, we scale cases and deaths by population.[^pop] State population data
is available inside `{epipredict}` as `state_census`.


```{r population-scaling}
dat <- left_join(
  x = dat, 
  y = state_census %>% select(pop, abbr), 
  by = c("geo_value" = "abbr"))

dat <- dat %>%
  mutate(cases = cases / pop * 1e5, # cases / 100K
         deaths = deaths / pop * 1e6) %>% # deaths / 1M
  select(-pop)
```

Now, use `epi_slide()`, to calculate trailing 7 day averages of cases and deaths.

```{r trailing-averages}
dat <- dat %>%
  group_by(geo_value) %>%
  epi_slide(cases = mean(cases), before = 6) %>%
  epi_slide(deaths = mean(deaths), before = 6) %>%
  ungroup()
```

## Finding possible features

Now we use `{epiprocess}` to examine which features might be useful for
forecasting (and which lags). This is not meant to be a comprehensive 
discussion of the type of exploratory data analysis that one should do when
examining predictive models, but at least a partial illustration.

First we split the data into "training" and "testing" sets. We're going to
(eventually) use a pretend forecast data of `2021-11-30`, so that's where
we'll split.

```{r train-test-split}
training <- dat %>% filter(time_value <= "2021-12-01")
```

Suppose we want to make a forecast for 2 weeks ahead. Then it's reasonable
to investigate which features correlate well with that target. We should
only look at the training data. Unlike the lecture, we'll group these by state, 
and examine the density across states. More mass toward larger values would 
suggest more utility for forecasting. For example,

```{r epi-cor, fig.align='center'}
# look at masking and deaths (where we moved deaths forward by 14 days)
c1 <- epi_cor(training, deaths, masking, cor_by = geo_value, dt1 = 14)

ggplot(c1, aes(cor)) +
  geom_density(fill = "cornflowerblue", colour = "cornflowerblue") +
  labs(x = "Correlation", y = "Density") +
  geom_vline(xintercept = 0)
```

Try adjusting the above chunk to examine other features that may be useful.
You could also try setting `method = "spearman"` to examine nonlinear
relationships. See `?epi_cor()`.

# Making a forecast

Now, we'll make a forecast, for all locations simultaneously. There are many
options to explore here, and we suggest you try! 



Finally, further down, we wrap it, to do multiple horizons simultaneously.
That code takes 4x as long to run. This is not a problem for this example
(the whole markdown compiles in < 10 seconds)
but if you try a different engine (like `"stan"`), it could take a really 
long time.

Below is some code to produce a forecast for 2 weeks ahead.

```{r forecast-1-horizon}

library(parsnip)
fcast <- arx_forecaster(
  epi_data = training,
  outcome = "deaths", 
  # Which ones would you use?
  predictors = c("deaths", "cases", "masking"), 
  # You could try other trainers; see https://parsnip.tidymodels.org. While
  # we've tried lots of them, some may fail and some aren't appropriate (e.g.
  # multinom_reg()). If you have {ranger} installed, try `rand_forest()`.
  # 
  # If you want to use `quantile_reg()` you need to specify the levels in the
  # trainer and below (and include .5 for a point forecast)
  # (we're going to fix this https://github.com/cmu-delphi/epipredict/issues/208)
  trainer = linear_reg(),
  arx_args_list(
    # the same for all predictors; to differentiate, use a list of vectors, one
    # per predictor (in the same order)
    lags = c(0, 7, 14),
    ahead = 14L, # try whatever you like
    forecast_date = as.Date("2021-12-01"),
    # this should be > ahead + max(lags)
    # You'll actually have n_training - (ahead + max(lags)) observations
    # per location for training
    n_training = Inf,
    # For the sake of our plotting below,
    # be sure to include 0.1 and 0.9. But you can add more
    levels = c(0.1, 0.9),
    # you could set this to "geo_value" if you thought that different
    # states should have wider/narrower intervals, but since
    # we're forecasting rates, this probably doesn't matter
    # (other things equal, we'd need more training data, if we did)
    quantile_by_key = character(0L)
  )
)
```

The warning here is because our data version is 1.5 years after the forecast date.
So performance is likely better than it would be for a production forecast.

Now, generate a plot for a few locations with the testing data as well.

```{r plot-fun}
plot_fcast <- function(fcasts, geos) {
  dat <- dat %>% filter(geo_value %in% geos)
  fcasts <- fcasts$predictions %>% 
    filter(geo_value %in% geos) %>%
    pivot_quantiles(contains("distn"))
  
  ggplot(fcasts) +
    geom_errorbar(
      aes(target_date, ymin = `0.1`, ymax = `0.9`, colour = geo_value)) +
    geom_point(aes(target_date, .pred, colour = geo_value), size = 2) +
    geom_line(data = dat, aes(time_value, deaths)) +
    geom_vline(aes(xintercept = forecast_date), linewidth = 1) +
    facet_wrap(~geo_value, scales = "free_y") +
    scale_colour_viridis_d(name = "") +
    theme(legend.position = "none") +
    labs(x = "Date", y = "Deaths per 1M inhabitants")
}
```

```{r make-the-plot, fig.align='center', fig.width=8, fig.height=6}
selected_geos <- c("ca", "ut", "pa", "ga")
plot_fcast(fcast, selected_geos)
```

---

If there's extra time, you could also try playing with the 
code below that makes multiple forecasts simultaneously and plots the result.

```{r forecast-4-horizons, warning=FALSE}
more_forecasts <- map(
  .x = 1:4 * 7, 
  ~ arx_forecaster(
    epi_data = training,
    outcome = "deaths", 
    predictors = c("deaths", "cases"),
    trainer = linear_reg(),
    arx_args_list(
      lags = c(0, 7, 14),
      ahead = .x,
      forecast_date = as.Date("2021-12-01"),
      levels = c(0.1, 0.25, 0.75, 0.9),
    )
  )$predictions) %>%
  bind_rows()
```

```{r make-trajectory-plot, echo=FALSE, fig.align='center', fig.width=8, fig.height=6}
plot_trajectory <- function(fcasts, geos) {
  dat <- dat %>% filter(geo_value %in% geos)
  fcasts <- fcasts %>%
    filter(geo_value %in% geos) %>%
    pivot_quantiles(contains("distn"))
  
  ggplot(fcasts) +
    geom_ribbon(
      aes(target_date, ymin = `0.1`, ymax = `0.9`, fill = geo_value),
      alpha = .4) +
    geom_ribbon(
      aes(target_date, ymin = `0.25`, ymax = `0.75`, fill = geo_value),
      alpha = .6) +
    geom_point(aes(target_date, .pred, colour = geo_value), size = 2) +
    geom_line(aes(target_date, .pred, colour = geo_value)) +
    geom_line(data = dat, aes(time_value, deaths)) +
    geom_vline(aes(xintercept = forecast_date)) +
    facet_wrap(~geo_value, scales = "free_y") +
    scale_colour_viridis_d(name = "") +
    scale_fill_viridis_d(name = "") +
    theme(legend.position = "none") +
    labs(x = "Date", y = "Deaths per 1M inhabitants")
}

plot_trajectory(more_forecasts, selected_geos)
```




[^pop]: In `{epipredict}` this preprocessing would be done with 
`step_population_scaling()` and inverted at predict time with `layer_population_scaling()`.
